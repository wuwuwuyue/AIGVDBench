# AIGVDBench: Comprehensive Benchmark for AI-Generated Video Detection

<div align="center">
<img src="fig/fig1.png" alt="AIGVDBench Cover" width="=800"/>
</div>

## ğŸ“– Overview
AIGVDBench is the a large-scale, high-quality benchmark for AI-generated video detection, designed to address key limitations in dataset scale and model diversity. This benchmark comprises over 440,000 videos from 31 generation models, covering text-to-video, image-to-video, video-to-video, and real-world scenarios.

**Key Contributions:**
- ğŸ¯ Standardized benchmark construction pipeline ensuring representativeness and reproducibility  
- ğŸ“Š High-quality benchmark with 422K+ videos from diverse generation models  
- ğŸ”¬ Systematic evaluation of 33 detectors across 4 categories with 8 in-depth analyses  
- ğŸ’¡ Identification of 4 novel findings for future research directions  

---

## ğŸš€ Key Features

### Dataset Scale and Diversity

## Comparison with Existing AI-Generated Video Detection Datasets

AIGVDBench significantly surpasses all others in scale, content diversity, coverage of generation tasks, and variety of video generation models. It comprises videos generated by **31** distinct models (**20** open-source and **11** closed-source), spanning a wide range of tasks including **23** text-to-video (T2V), **6** image-to-video (I2V), and **2** video-to-video (V2V) models, with the closed-source models being primarily but not exclusively focused on T2V generation.

| Dataset | Publication | Latest Models | Methods | Open Source | Closed Source | T2V | I2V | V2V | Real World | Generated Videos | Content Diversity |
|---------|------------|---------------|---------|-------------|---------------|-----|-----|-----|------------|-----------------|-----------------|
| GVD | PRCV'24 | Sora (2024.2) | 11 | 3 | 8 | 8 | 3 | - | âœ… | 11.6k | âŒ |
| GVF | ICME'25 | Kling (2024.6) | 9 | 4 | 5 | 9 | - | - | âœ… | 4.2k | âŒ |
| GenVideo | ArXiv'24 | OpenSora (2024.3) | 20 | 14 | 6 | 16 | 4 | - | - | 100k | âŒ |
| GenVidBench | ArXiv'25 | Mora (2024.3) | 8 | 7 | 1 | 6 | 2 | - | - | 109.2k | âŒ |
| GenBuster-200K | ArXiv'25 | EasyAnimate (2025.1) | 12 | 4 | 8 | 8 | - | - | - | 101.1k | âŒ |
| GenWorld | ArXiv'25 | Cosmos (2025.1) | 10 | 10 | - | 7 | 2 | 1 | - | 89.4k | âŒ |
| **Ours (AIGVDBench)** | - | Open-Sora (2025.3) | **31** | **20** | **11** | **23** | **6** | **2** | âœ… | **422k** | âœ… |


**Table Notes:**

- **Latest Models**: The year when the latest video generation model was added to the dataset.  
- **Methods**: Number of different video generation models used to generate videos in the dataset.  
- **Model Type**: Classified into open-source and closed-source.  
- **Generation Tasks**: Based on generation task types, models are categorized into text-to-video (T2V), image-to-video (I2V), and video-to-video (V2V).  
- **Generated Videos**: Number of generated videos in the dataset.  
- **Real World**: Indicates whether the dataset includes videos from real-user scenarios.  
- **Content Diversity:** âœ… = automated / balanced, âŒ = manual / not balanced



### Technical Innovations
- **Attribute Balancing Algorithm:** Multi-label prompt selection ensuring balanced content distribution  
- **Compression Format Unification:** All videos standardized to H.264 codec  
- **Real-world Simulation:** Closed-source model test sets simulate authentic user scenarios  
- **Comprehensive Evaluation:** 1,500+ evaluations across 33 detectors and 31 generation models  

---

## ğŸ“Š Dataset Details

**Included Generation Models**

- **Open-Source Models (20):** Open-Sora, RepVideo, AccVideo, CogVideoX, EasyAnimate, Wan2.1, VideoCrafter, Pyramid-Flow, IPOC, Hunyuan, LTX, AnimateDiff, SEINE, SVD, and more  
- **Closed-Source Models (11):** Sora, Kling, Gen2, Gen3, Causvid, Luma, Pika, Vidu, Wan, Jimeng, OpenSora  

**Data Statistics**

| Category | Open-Source Models | Closed-Source Models |
|----------|------------------|--------------------|
| Training Videos | 14,000 per model | - |
| Test Videos | 3,000 per model | 2,000 per model |
| Real Videos | 14,000 | 2,000 per model |

---

## ğŸ“ˆ Benchmark Results

**Detector Categories Evaluated:**
- **Video Classification Models:** MViTv2, UniFormer, VideoSwin, VideoMAE, TSM, SlowFast, TimeSformer, I3D, X3D, UniFormerV2  
- **AI-Generated Image Detection Models:** NPR, FreDect, Fusing, Gram-Net, CNNSpot, D3, ForgeLens, Effort, UnivFD  
- **AI-Generated Video Detection Models:** DeMamba, DeCoF  
- **Vision-Language Models:** Emu3-Stage1, FastVLM, DeepseekVL, LLaVA, Kimi-VL, Qwen2.5-VL, InternVL3  

**Key Findings:**
1. AI-generated video detection remains challenging, with promising directions including classic video classification, frame-level image detection, temporal artifact exploitation, and vision-language models for interpretable detection.  
2. Improvements in video generation model quality do not ensure reduced detectability or better detector generalization.  
3. The type of generation task significantly impacts detector performance.  
4. Current vision-language models lack reliable capability for detecting AI-generated videos.  

---

## ğŸ”¬ In-Depth Analysis

- **Open vs. Closed Source Model Detectability:** Closed-source models show significantly higher detection difficulty.  
- **Generation Task Impact:** Detection performance varies across T2V, I2V, V2V tasks; V2V content is most challenging.
> For more detailed analysis, please refer to our [paper](https://huggingface.co/datasets/AIGVDBench/AIGVDBench/tree/main).

---

## ğŸ“ Project Structure
## ğŸ“Š Data Access

- Full Dataset available via download links  
---

## ğŸ“œ Citation

If you use AIGVDBench in your research, please cite:

```bibtex

